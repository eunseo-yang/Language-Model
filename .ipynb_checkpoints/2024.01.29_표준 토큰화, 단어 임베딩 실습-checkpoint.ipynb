{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806a5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3964dfd1",
   "metadata": {},
   "source": [
    "## 1. 표준 토큰화\n",
    "\n",
    "자연어 처리에 사용되는 nltk를 사용하면 말뭉치, 토큰 생성, 형태소 분석, 품사 태깅을 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f0f16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Model-based RL don't need a value function for the policy.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7944e0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'function', 'for', 'the', 'policy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a6015",
   "metadata": {},
   "source": [
    "## 2. 어간 추출 및 표제어 추출\n",
    "\n",
    "토큰화한 단계에서 분석을 하게 되면 품사의 다양한 형태나 다양한 시제에 따라 같은 단어가 다르게 토큰화될 수 있어서 불리하다. 여러 어간 추출 패키지가 존재하니 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ddc71",
   "metadata": {},
   "source": [
    "### 2.1 어간 추출 (PorterStemmer) vs 표제어 추출 (LancasterStemmer)\n",
    "\n",
    "* 어간 추출: 품사 태깅 (x) 동사, 명사 구분 불가\n",
    "* 표제어 추출: 품사 태깅 (o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1639904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmer,어간 추출: ['eat', 'ate', 'eaten', 'eat']\n",
      "LancasterStemmer,표제어 추출: ['eat', 'at', 'eat', 'eat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "stem1 = PorterStemmer()\n",
    "stem2 = LancasterStemmer() # 품사 맞춤\n",
    "words = [\"eat\",\"ate\",\"eaten\",\"eating\"]\n",
    "print(\"PorterStemmer,어간 추출:\",[stem1.stem(w) for w in words])\n",
    "print(\"LancasterStemmer,표제어 추출:\",[stem2.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c527ee7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/yang-\n",
      "[nltk_data]     eunseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Lemmatizer: ['eat', 'eat', 'eat', 'eat']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import WordNetLemmatizer # 시제 맞춤\n",
    "nltk.download(\"wordnet\")\n",
    "lemm = WordNetLemmatizer()\n",
    "words = [\"eat\",\"ate\",\"eaten\",\"eating\"]\n",
    "print(\"WordNet Lemmatizer:\",[lemm.lemmatize(w,pos=\"v\") for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25044c6",
   "metadata": {},
   "source": [
    "## 3. 불용어 제거\n",
    "\n",
    "전체적인 의미에 방해되지 않는 선에서 불필요한 단어, 잘 쓰이지 않는 단어는 제외한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a077c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yang-\n",
      "[nltk_data]     eunseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english')[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a8a679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['We', 'should', 'all', 'study', 'hard', 'for', 'the', 'exam', '.']\n",
      "After: ['We', 'study', 'hard', 'exam', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yang-\n",
      "[nltk_data]     eunseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "input_sentence = \"We should all study hard for the exam.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(input_sentence)\n",
    "result = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        result.append(w)\n",
    "        \n",
    "print(\"Before:\",word_tokens)\n",
    "print(\"After:\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b074a7",
   "metadata": {},
   "source": [
    "## 4. 정수 인코딩 및 Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba018046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course:English, Number:0\n",
      "Course:Math, Number:1\n",
      "Course:Science, Number:2\n"
     ]
    }
   ],
   "source": [
    "mylist = [\"English\",\"Math\",\"Science\"]\n",
    "for n, name in enumerate(mylist):\n",
    "    print(\"Course:{}, Number:{}\".format(name,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b8e298d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cup', 8), ('July', 6), ('piano', 4), ('apple', 2), ('orange', 1)]\n",
      "{'cup': 1, 'July': 2, 'piano': 3, 'apple': 4, 'orange': 5}\n"
     ]
    }
   ],
   "source": [
    "# 단어마다의 등장 빈도 수\n",
    "vocab = {'apple':2,'July':6,'piano':4,'cup':8,'orange':1}\n",
    "vocab_sort = sorted(vocab.items(),key=lambda x:x[1], reverse=True)\n",
    "print(vocab_sort)\n",
    "\n",
    "word2inx = {word[0]:index + 1 for index,word in enumerate(vocab_sort)}\n",
    "print(word2inx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5e4e1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model-based': 0, 'RL': 1, 'do': 2, \"n't\": 3, 'need': 4, 'a': 5, 'value': 6, 'function': 7, 'for': 8, 'the': 9, 'policy': 10, ',': 11, 'but': 12, 'some': 13, 'of': 14, 'algorithms': 15, 'have': 16, '.': 17} \n",
      "\n",
      "[2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Model-based RL don't need a value function for the policy, but some of Model-based RL algorithms do have a value function.\"\n",
    "\n",
    "token_text = tokenizer.tokenize(text)\n",
    "word2inx = {}\n",
    "Bow = []\n",
    "for word in token_text:\n",
    "    if word not in word2inx.keys():\n",
    "        word2inx[word] = len(word2inx)\n",
    "        Bow.insert(len(word2inx)-1,1)\n",
    "    else:\n",
    "        inx = word2inx.get(word)\n",
    "        Bow[inx] += 1\n",
    "print(word2inx,\"\\n\")\n",
    "print(Bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a413f",
   "metadata": {},
   "source": [
    "## 문장 인코딩까지 마무리\n",
    "\n",
    "문장 → 토큰화 → 정제 → 추출 → 인코딩 (정수 인코딩, high freq 위주로 실습)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a306a6",
   "metadata": {},
   "source": [
    "## 5. 유사도 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c512888d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.7071067811865475 0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_sim(A,B):\n",
    "    return np.dot(A,B) / (np.linalg.norm(A)*np.linalg.norm(B))\n",
    "\n",
    "a = [1,0,0,1]\n",
    "b = [0,1,1,0]\n",
    "c = [1,1,1,1]\n",
    "print(cos_sim(a,b), cos_sim(b,c), cos_sim(c,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23280df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "def leven(text1, text2):\n",
    "    len1 = len(text1) + 1\n",
    "    len2 = len(text2) + 1\n",
    "    \n",
    "    # 2차원 표를 채우는 것 (세로열, 가로열)\n",
    "    sim_array = np.zeros((len1, len2))\n",
    "    sim_array[:,0] = np.linspace(0,len1-1,len1)\n",
    "    sim_array[0,:] = np.linspace(0,len2-1,len2)    \n",
    "    \n",
    "    for i in range(1, len1):\n",
    "        for j in range(1, len2):\n",
    "            add_char = sim_array[i-1,j] + 1\n",
    "            sub_char = sim_array[i,j-1] + 1\n",
    "            if text1[i-1] == text2[j-1]:\n",
    "                mod_char = sim_array[i-1,j-1]\n",
    "            else:\n",
    "                mod_char = sim_array[i-1,j-1] + 1\n",
    "            sim_array[i,j] = min([add_char,sub_char,mod_char])\n",
    "    return sim_array[-1,-1] #가장 오른쪽 끝\n",
    "    \n",
    "print(leven('데이터마이닝','데이타마닝'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b7479",
   "metadata": {},
   "source": [
    "## 6. Word2Vec: CBoW, SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd25204",
   "metadata": {},
   "outputs": [],
   "source": [
    "3:10:00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
